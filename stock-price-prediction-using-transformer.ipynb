{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11005205,"sourceType":"datasetVersion","datasetId":6851115},{"sourceId":11005228,"sourceType":"datasetVersion","datasetId":6851134}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:04:26.740843Z","iopub.execute_input":"2025-03-13T06:04:26.741170Z","iopub.status.idle":"2025-03-13T06:04:26.745979Z","shell.execute_reply.started":"2025-03-13T06:04:26.741147Z","shell.execute_reply":"2025-03-13T06:04:26.745070Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:04:26.750149Z","iopub.execute_input":"2025-03-13T06:04:26.750414Z","iopub.status.idle":"2025-03-13T06:04:26.763804Z","shell.execute_reply.started":"2025-03-13T06:04:26.750392Z","shell.execute_reply":"2025-03-13T06:04:26.763077Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"class StockDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.float32)  # Keep on CPU\n        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # Keep on CPU\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]  # Do NOT move to GPU here\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:04:27.176129Z","iopub.execute_input":"2025-03-13T06:04:27.176395Z","iopub.status.idle":"2025-03-13T06:04:27.190876Z","shell.execute_reply.started":"2025-03-13T06:04:27.176367Z","shell.execute_reply":"2025-03-13T06:04:27.190101Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"class StockTransformer(nn.Module):\n    def __init__(self, input_dim, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n        super(StockTransformer, self).__init__()\n        self.embedding = nn.Linear(input_dim, embed_dim)  # Feature embedding\n        self.encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout, batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n        self.fc = nn.Linear(embed_dim, 1)  # Output layer (predict next Close price)\n\n    def forward(self, x):\n        x = self.embedding(x)  # Linear projection\n        x = self.transformer_encoder(x)\n        x = x.mean(dim=1)  # Global average pooling\n        x = self.fc(x)  # Regression output\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:04:50.804799Z","iopub.execute_input":"2025-03-13T06:04:50.805124Z","iopub.status.idle":"2025-03-13T06:04:50.810790Z","shell.execute_reply.started":"2025-03-13T06:04:50.805088Z","shell.execute_reply":"2025-03-13T06:04:50.810090Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"def create_sequences(data, seq_length=30):\n    \"\"\"Converts time series data into sequences for Transformer.\"\"\"\n    sequences, targets = [], []\n    \n    for i in range(len(data) - seq_length):\n        seq = data.iloc[i:i+seq_length]  # Get sequence window\n        target = data.iloc[i+seq_length][\"close\"]  # Predict next close price\n        \n        sequences.append(seq[features].values)  # Extract numerical features\n        targets.append(target)  # Store target\n    \n    return np.array(sequences), np.array(targets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:04:27.168702Z","iopub.execute_input":"2025-03-13T06:04:27.168955Z","iopub.status.idle":"2025-03-13T06:04:27.174011Z","shell.execute_reply.started":"2025-03-13T06:04:27.168917Z","shell.execute_reply":"2025-03-13T06:04:27.173178Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"# Hyperparameters\nSEQ_LEN = 30  # Number of past days used for prediction\nPRED_LEN = 1  # Predict the next day's price\nD_MODEL = 64  # Transformer model dimension\nNHEAD = 4  # Multi-head attention heads\nNUM_LAYERS = 3  # Transformer layers\nBATCH_SIZE = 32\nEPOCHS = 20\nLR = 0.001","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:04:26.764905Z","iopub.execute_input":"2025-03-13T06:04:26.765250Z","iopub.status.idle":"2025-03-13T06:04:26.778977Z","shell.execute_reply.started":"2025-03-13T06:04:26.765216Z","shell.execute_reply":"2025-03-13T06:04:26.778152Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"data_dir = \"/kaggle/input/historical-dataset/historical_data\"  # Path to your dataset folder\nall_data = []\n\n# Load and process each CSV file\nfor file in os.listdir(data_dir):\n    if file.endswith(\".csv\"):\n        file_path = os.path.join(data_dir, file)\n        stock_name = file.replace(\".csv\", \"\")  # Extract stock ticker\n        \n        # Load data\n        df = pd.read_csv(file_path)\n        \n        # Add stock name as a new column\n        df[\"Stock\"] = stock_name\n        \n        # Append to list\n        all_data.append(df)\n\n# Combine all stock data into a single DataFrame\nfinal_dataset = pd.concat(all_data, ignore_index=True)\nfinal_dataset[\"timestamp\"] = pd.to_datetime(final_dataset[\"timestamp\"])  # Convert to datetime format\nfinal_dataset = final_dataset.sort_values(by=[\"timestamp\"])  # Sort by date","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:04:26.780725Z","iopub.execute_input":"2025-03-13T06:04:26.780936Z","iopub.status.idle":"2025-03-13T06:04:27.115244Z","shell.execute_reply.started":"2025-03-13T06:04:26.780915Z","shell.execute_reply":"2025-03-13T06:04:27.114494Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"final_dataset = final_dataset.drop(columns=[\"oi\"])\nprint(final_dataset.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:04:27.116543Z","iopub.execute_input":"2025-03-13T06:04:27.116912Z","iopub.status.idle":"2025-03-13T06:04:27.127143Z","shell.execute_reply.started":"2025-03-13T06:04:27.116873Z","shell.execute_reply":"2025-03-13T06:04:27.126422Z"}},"outputs":[{"name":"stdout","text":"                      timestamp     open     high      low    close  volume  \\\n31211 2025-01-01 09:15:00+05:30   308.70   309.15   307.05   307.25  325458   \n17198 2025-01-01 09:15:00+05:30  2280.00  2305.00  2280.00  2299.00  108780   \n2547  2025-01-01 09:15:00+05:30   292.30   293.65   291.50   293.15  422483   \n11465 2025-01-01 09:15:00+05:30   601.50   603.75   596.50   599.55  572072   \n5732  2025-01-01 09:15:00+05:30  1214.85  1217.90  1213.50  1216.20  423841   \n\n            Stock  \n31211   POWERGRID  \n17198  ASIANPAINT  \n2547         BPCL  \n11465    HINDALCO  \n5732     RELIANCE  \n","output_type":"stream"}],"execution_count":89},{"cell_type":"code","source":"label_encoder = LabelEncoder()\nfinal_dataset[\"Stock\"] = label_encoder.fit_transform(final_dataset[\"Stock\"])\nscaler = MinMaxScaler()\nfeatures = [\"open\", \"high\", \"low\", \"close\", \"volume\"]  # Feature columns\nfinal_dataset[features] = scaler.fit_transform(final_dataset[features])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:04:27.128048Z","iopub.execute_input":"2025-03-13T06:04:27.128398Z","iopub.status.idle":"2025-03-13T06:04:27.145870Z","shell.execute_reply.started":"2025-03-13T06:04:27.128363Z","shell.execute_reply":"2025-03-13T06:04:27.144982Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"# Create sequences\nseq_length = 30  # Number of past days to look at\nX, y = create_sequences(final_dataset, seq_length)\n\n# 游릭 Step 5: Train-Test Split\ntrain_size = int(0.8 * len(X))\nX_train, X_test = X[:train_size], X[train_size:]\ny_train, y_test = y[:train_size], y[train_size:]\n\n# Print dataset shapes\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_test shape: {y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:04:27.191873Z","iopub.execute_input":"2025-03-13T06:04:27.192187Z","iopub.status.idle":"2025-03-13T06:04:50.778527Z","shell.execute_reply.started":"2025-03-13T06:04:27.192165Z","shell.execute_reply":"2025-03-13T06:04:50.777740Z"}},"outputs":[{"name":"stdout","text":"X_train shape: (24945, 30, 5)\nX_test shape: (6237, 30, 5)\ny_train shape: (24945,)\ny_test shape: (6237,)\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"# 游릭 Convert NumPy Arrays to PyTorch Tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Make y 2D\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n\n# 游릭 Create PyTorch DataLoader\nbatch_size = 32\ntrain_dataset = StockDataset(X_train_tensor, y_train_tensor)\ntest_dataset = StockDataset(X_test_tensor, y_test_tensor)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=256,  # Try increasing batch size\n    shuffle=True,\n    pin_memory=True,  # Keep CPU tensors ready for fast transfer\n    num_workers=4  # Adjust based on Kaggle CPU cores (try 2, 4, or 8)\n)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:07:23.681862Z","iopub.execute_input":"2025-03-13T06:07:23.682191Z","iopub.status.idle":"2025-03-13T06:07:23.707211Z","shell.execute_reply.started":"2025-03-13T06:07:23.682166Z","shell.execute_reply":"2025-03-13T06:07:23.705904Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-93-3be1e7224d34>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.X = torch.tensor(X, dtype=torch.float32)  # Keep on CPU\n<ipython-input-93-3be1e7224d34>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # Keep on CPU\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"# 游릭 Initialize Model, Loss, Optimizer\ninput_dim = X_train.shape[2]  # Number of features\nembed_dim = 64\nnum_heads = 4\nff_dim = 128\nnum_layers = 3\ndropout = 0.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:04:50.811578Z","iopub.execute_input":"2025-03-13T06:04:50.811832Z","iopub.status.idle":"2025-03-13T06:04:50.829943Z","shell.execute_reply.started":"2025-03-13T06:04:50.811801Z","shell.execute_reply":"2025-03-13T06:04:50.829247Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"model = StockTransformer(input_dim, embed_dim, num_heads, ff_dim, num_layers, dropout).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.AdamW(model.parameters(), lr=0.001)  # 游 AdamW for better GPU optimization\n\nnum_epochs = 50\n\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        X_batch, y_batch = X_batch.to(device, non_blocking=True), y_batch.to(device, non_blocking=True)  # Move to GPU here\n        predictions = model(X_batch)  # Forward pass\n        loss = criterion(predictions, y_batch)\n        loss.backward()  # Backpropagation\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:07:28.283527Z","iopub.execute_input":"2025-03-13T06:07:28.283892Z","execution_failed":"2025-03-13T06:07:48.476Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([256, 1, 1])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50, Loss: 0.055620\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([113, 1, 1])) that is different to the input size (torch.Size([113, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/50, Loss: 0.044490\nEpoch 3/50, Loss: 0.044499\nEpoch 4/50, Loss: 0.044595\nEpoch 5/50, Loss: 0.044339\nEpoch 6/50, Loss: 0.044773\nEpoch 7/50, Loss: 0.045343\nEpoch 8/50, Loss: 0.045010\nEpoch 9/50, Loss: 0.044600\nEpoch 10/50, Loss: 0.044637\nEpoch 11/50, Loss: 0.044352\nEpoch 12/50, Loss: 0.044433\nEpoch 13/50, Loss: 0.044476\nEpoch 14/50, Loss: 0.044388\nEpoch 15/50, Loss: 0.044537\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# 游릭 Evaluate the Model on GPU\nmodel.eval()\nwith torch.no_grad():\n    total_loss = 0\n    for X_batch, y_batch in test_loader:\n        predictions = model(X_batch)\n        loss = criterion(predictions, y_batch)\n        total_loss += loss.item()\n\nprint(f\"Test Loss: {total_loss / len(test_loader):.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:06:13.875450Z","iopub.status.idle":"2025-03-13T06:06:13.875856Z","shell.execute_reply":"2025-03-13T06:06:13.875686Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Prediction Pipeline","metadata":{}},{"cell_type":"code","source":"def prepare_input(new_data, stock_encoder, scaler):\n    \"\"\"\n    Prepares stock market input data:\n    - Encodes stock names numerically\n    - Converts timestamps to numerical format\n    - Normalizes all features\n    - Converts to PyTorch tensor and moves to GPU\n    \"\"\"\n    # Convert timestamp to Unix time (seconds since epoch)\n    new_data[\"timestamp\"] = pd.to_datetime(new_data[\"timestamp\"]).astype(int) // 10**9  \n\n    # Encode stock name using provided mapping\n    new_stock_data[\"stock\"] = new_stock_data[\"stock\"].map(stock_encoder)\n\n    # Select relevant columns for model input\n    feature_columns = [\"timestamp\", \"stock\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n    input_data = new_data[feature_columns].values  # Convert to numpy array\n\n    # Normalize using same scaler as training\n    input_data_scaled = scaler.transform(input_data)\n\n    # Convert to PyTorch tensor and move to GPU\n    input_tensor = torch.tensor(input_data_scaled, dtype=torch.float32).unsqueeze(0).to(\"cuda\")  \n    return input_tensor\n\ndef predict_stock_price(model, new_data_tensor):\n    \"\"\"\n    Predicts stock price for a single input.\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        prediction = model(new_data_tensor)\n    return prediction.cpu().numpy()\n\ndef inverse_transform_prediction(predicted_price, scaler, feature_index=5):  \n    \"\"\"\n    Converts normalized predictions back to actual stock prices.\n    - Assumes 'close' price is at index 5 in the scaler\n    \"\"\"\n    temp = np.zeros((predicted_price.shape[0], scaler.n_features_in_))  \n    temp[:, feature_index] = predicted_price.squeeze()  # Put predictions in the correct column\n\n    actual_price = scaler.inverse_transform(temp)[:, feature_index]  # Extract actual price values\n    return actual_price\n\ndef predict_future_prices(model, new_data, stock_encoder, scaler, days=10):\n    \"\"\"\n    Predicts stock prices for multiple future days using a rolling window.\n    \"\"\"\n    future_predictions = []\n    input_seq = prepare_input(new_data, stock_encoder, scaler)\n\n    for _ in range(days):\n        pred = predict_stock_price(model, input_seq)\n        future_predictions.append(pred)\n\n        # Shift window: Remove first step, add new prediction\n        pred_tensor = torch.tensor(pred, dtype=torch.float32).unsqueeze(0).to(\"cuda\")\n        input_seq = torch.cat([input_seq[:, 1:, :], pred_tensor], dim=1)\n\n    # Convert predictions back to actual stock prices\n    actual_predictions = inverse_transform_prediction(np.array(future_predictions), scaler)\n    return actual_predictions\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stock_encoder = {stock: idx for idx, stock in enumerate(label_encoder.classes_)}\n\n# Example new data\nnew_stock_data = pd.DataFrame({\n    \"timestamp\": [\"2025-01-01 09:15:00+05:30\"],\n    \"stock\": [\"RELIANCE\"],  \n    \"open\": [1214.85], \"high\": [1217.90], \"low\": [1213.50], \"close\": [1216.20], \"volume\": [423841]\n})\n\nnew_data_tensor = prepare_input(new_stock_data, stock_encoder, scaler)\n\n# Single-day prediction\npredicted_price = predict_stock_price(model, new_data_tensor)\nactual_price = inverse_transform_prediction(predicted_price, scaler)\nprint(\"Predicted Stock Price:\", actual_price)\n\n# Multi-day prediction\nfuture_prices = predict_future_prices(model, new_stock_data, stock_encoder, scaler, days=10)\nprint(\"Future Stock Predictions:\", future_prices)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}