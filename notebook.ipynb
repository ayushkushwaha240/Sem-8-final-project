{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preprocessing\n",
    "# we'll use close and volume\n",
    "# embeddings of different stocks name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset path\n",
    "data_dir = \"/Users/ayushkushwaha/Desktop/Sem-8-final-project/dataset\"\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15  # Test ratio will be 1 - train_ratio - val_ratio\n",
    "\n",
    "# Iterate through each CSV file\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        df = pd.read_csv(file_path, parse_dates=['Date'])  # Ensure 'Date' column is parsed as datetime\n",
    "        df = df.sort_values(by='Date')  # Sort by date if not already sorted\n",
    "        \n",
    "        # Split indices\n",
    "        train_size = int(len(df) * train_ratio)\n",
    "        val_size = int(len(df) * val_ratio)\n",
    "        \n",
    "        train_data = df.iloc[:train_size]\n",
    "        val_data = df.iloc[train_size:train_size + val_size]\n",
    "        test_data = df.iloc[train_size + val_size:]\n",
    "\n",
    "        # Save or process further\n",
    "        train_data.to_csv(f\"dataset/splits/train_{file}\", index=False)\n",
    "        val_data.to_csv(f\"dataset/splits/val_{file}\", index=False)\n",
    "        test_data.to_csv(f\"dataset/splits/test_{file}\", index=False)\n",
    "\n",
    "        print(f\"Processed {file}: Train({len(train_data)}), Val({len(val_data)}), Test({len(test_data)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "SEQ_LEN = 30  # Number of past days used for prediction\n",
    "PRED_LEN = 1  # Predict the next day's price\n",
    "D_MODEL = 64  # Transformer model dimension\n",
    "NHEAD = 4  # Multi-head attention heads\n",
    "NUM_LAYERS = 3  # Transformer layers\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LR = 0.001\n",
    "\n",
    "# Load dataset\n",
    "data_dir = \"dataset/historical_data\"\n",
    "\n",
    "# Function to load and preprocess stock data\n",
    "def load_stock_data(file_path):\n",
    "    df = pd.read_csv(file_path, parse_dates=['Date'])\n",
    "    df = df.sort_values(by='Date')\n",
    "    df['Return'] = df['Close'].pct_change()  # Use returns instead of raw prices\n",
    "    df = df.dropna()\n",
    "    return df[['Return']].values  # Return as NumPy array\n",
    "\n",
    "# Prepare dataset for Transformer\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data, seq_len=SEQ_LEN, pred_len=PRED_LEN):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        y = self.data[idx + self.seq_len:idx + self.seq_len + self.pred_len]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Load all stock data and concatenate\n",
    "all_data = []\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        stock_data = load_stock_data(file_path)\n",
    "        all_data.append(stock_data)\n",
    "\n",
    "# Stack all data\n",
    "all_data = np.concatenate(all_data, axis=0)\n",
    "\n",
    "# Train-validation-test split\n",
    "train_size = int(len(all_data) * 0.7)\n",
    "val_size = int(len(all_data) * 0.15)\n",
    "train_data = all_data[:train_size]\n",
    "val_data = all_data[train_size:train_size + val_size]\n",
    "test_data = all_data[train_size + val_size:]\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = StockDataset(train_data)\n",
    "val_dataset = StockDataset(val_data)\n",
    "test_dataset = StockDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_model=D_MODEL, nhead=NHEAD, num_layers=NUM_LAYERS, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, PRED_LEN)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch, features)\n",
    "        out = self.transformer_encoder(x)\n",
    "        out = out[-1]  # Take the last output for prediction\n",
    "        return self.fc(out)\n",
    "\n",
    "# Initialize model\n",
    "model = TransformerModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, epochs=EPOCHS):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader)\n",
    "\n",
    "# Test the model\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    preds, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            preds.append(y_pred.cpu().numpy())\n",
    "            actuals.append(y.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(preds), np.concatenate(actuals)\n",
    "\n",
    "# Get predictions\n",
    "preds, actuals = evaluate_model(model, test_loader)\n",
    "print(f\"Test MSE: {np.mean((preds - actuals)**2):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customdataset(Dataset):\n",
    "    def __init__(self,data) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, ff_dim, output_dim,transformer_num = 2, dropout=0.1 ):\n",
    "        super(TransformerModel,self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, model_dim)\n",
    "        self.transformers = nn.Module()\n",
    "        for _ in range(transformer_num):\n",
    "            self.transformers.append(\n",
    "                nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dropout=dropout, batch_first=True),\n",
    "                num_layers=n_layers\n",
    "            )\n",
    "        self.fc = nn.Linear(d_model,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.fc_out(x[:, -1, :])  # Output from the last token\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 25\n",
    "lr = 0.001\n",
    "transformer_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = customdataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size= batch_size, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, Loss, Optimizer\n",
    "model = StockTransformer(D_MODEL, N_HEADS, N_LAYERS, DROPOUT,transformer_num = transformer_num)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.unsqueeze(-1), targets.unsqueeze(-1)  # Add feature dim\n",
    "        tgt_inputs = torch.zeros_like(targets)  # Decoder input (can be improved)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, tgt_inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
